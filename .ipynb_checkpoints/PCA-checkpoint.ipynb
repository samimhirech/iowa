{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal component analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reminder\n",
    "Principal component analysis (PCA) is a technique used in \n",
    "- approximation,\n",
    "- signal processing (noise reduction, compression…) \n",
    "\n",
    "but which can also lead to useful results in data analysis with a few aims :\n",
    "- visualisation and data exploration\n",
    "- dimensionality reduction\n",
    "- pattern classification, and more specifically in unsupervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical basis\n",
    "The basis of the PCA is to find the linear representation of the data that minimise the quadratic error while using a limited number of projection.\n",
    "\n",
    "We assume that we have a set of multivariate datapoints $(\\mathbf{x}_{i})_{i \\in [[1, m]]}$ living in $\\mathbb{R}^{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Order 0 approximation\n",
    "The best _single point_ to represent it in term of quadratic error is its mean.\n",
    "\n",
    "That is the point $\\mathbf{m}$ that minimise $\\sum_{i=1}^{m} ||\\mathbf{m} - \\mathbf{x}_i||^2$ is the empirical mean of the distribution : $\\mathbf{m} = \\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Order 1, 1D approximation\n",
    "We try to perform a linear (order 1) approximation of the data in 1D (that is as a monovariate dataset). In other word we try to find the vector $\\mathbf{e}$ ($||\\mathbf{e}||=1$) such that :\n",
    "\n",
    "$$ \\sum_{i=1}^{m}|| \\mathbf{x}_i - (\\mathbf{m} + a_i\\mathbf{e})||^2 $$\n",
    "\n",
    "You have seen that the min of this quadratic error is attained for :\n",
    "$$a_i = <(\\mathbf{x}_i - \\mathbf{m}),\\mathbf{e}> $$\n",
    "and most interestingly that $\\mathbf{e}$ is an eigenvector attached to the largest eigenvalue of the scatter matrix of the dataset : \n",
    "$$\\mathbf{S} = \\sum_{i=1}^{m}(\\mathbf{x}_i - \\mathbf{m})(\\mathbf{x}_i - \\mathbf{m})^t$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Order 1, kD approximation\n",
    "We want now to minimise the quadratic error of a k-dimensional linear approximation of the dataset :\n",
    "\n",
    "$$ \\sum_{i=1}^{m}|| \\mathbf{x}_i - (\\mathbf{m} + \\sum_{j=1}^{k}a_{i,k}\\mathbf{e}_k)||^2 $$ \n",
    "\n",
    "\n",
    "We can extend the result of the 1D linear approximation. This leads to project the dataset on the (partial) basis made of the $k$ eigenvector corresponding to the $k$ largest eigenvalues of the dataset's scatter matrix $\\mathbf{S}$.\n",
    "\n",
    "As $\\mathbf{S}$ is a positive semi-definite matrix, the eigenvectors form a orthogonal basis. Hence the $(\\mathbf{e}_k)_{k\\in[[1, j]]}$ form a (partial) orthonormal basis of $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principal component analysis\n",
    "\n",
    "Performing principal component analysis is all about computing the scatter matrix from the dataset and then performing the eigenvector decomposition of this matrix.\n",
    "\n",
    "If $\\mathbf{X}_\\mathsf{c}$ is the matrix with row $i$ containing $(\\mathbf{x}_i - \\mathbf{m})$, that is the _centered data_, then one can compute $\\mathbf{S} = \\mathbf{X}_\\mathsf{c}^t\\mathbf{X}_\\mathsf{c}$ as a simple outer product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We still have to compute the eigenvector decomposition once $\\mathbf{S}$ is computed :\n",
    "\n",
    "$$ \\mathbf{S} = \\mathbf{P}^t \\mathbf{D} \\mathbf{P}$$\n",
    "\n",
    "With $\\mathbf{P}$ being a $n \\times n$ *orthogonal matrix*, that is $\\mathbf{P}^{-1} = \\mathbf{P}^t$. In other words a matrix corresponding the change into another orthonormal basis (considering the initial basis is itself orthonormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Isotropic or anisotropic data / homogenuos or heterogenuous dimensions\n",
    "Doing a PCA is essentially performing a rotation of the basis (changing from a orthonormal basis to another one) such that the scatter matrix, $\\mathbf{S}$ becomes diagonal.\n",
    "\n",
    "This rotation makes senses only if the dataset space is isotropic (we can consider that the canonical basis is already orthonormal), or in other words that the dataset space is made of homogenuous dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise the _rotation_ might have no sense since it require performing linear combination of value which are in different unit and even worse _dimension_ (mixing **m** with **kg**, or the like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quadratic error / isotropic loss\n",
    "Another way to understand the same question : PCA objective is to minimise **quadratic error**. The main specificity (and advantage) of **quadratic error** is that it is **invariant by rotation** of the basis.\n",
    "\n",
    "In other words, one would consider **quadratic error** mostly to benefit from an **isotropic loss function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The _iris_ dataset example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reading in the data : 150 samples of 3 different species of _iris_, each sample caracterised by 4 length (_in cm_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Getting the dataset :\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "iris = pandas.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "                       header=None,\n",
    "                       names=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'species'],\n",
    "                       sep=',')\n",
    "## A bit of displaying :\n",
    "print(iris.head())\n",
    "print(iris.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "notice that each variable is a length, in _cm_, hence we are in the case that all variables are of the same _physical dimension_, and _unit_. The last variable (_column_) is a (_dimension less_) classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Isolating the variables (columns 0:4) and computing the scatter matrix\n",
    "X = iris.iloc[:, 0:4].values\n",
    "## From the classification of each sample \n",
    "cl = iris.iloc[:, 4].values\n",
    "\n",
    "print(numpy.unique(cl))\n",
    "print(iris.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Some visualisation\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "bins=numpy.linspace(X[::, 0].min(),X[::, 0].max(), 20)\n",
    "\n",
    "for c in numpy.unique(cl) :\n",
    "    pyplot.hist(X[cl==c, 0], bins, alpha=0.5, label=c)\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.xlabel(iris.columns.values[0])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Plotting all 4 histograms\n",
    "fig, axes = pyplot.subplots(nrows=1, ncols=4, figsize=[15,4])\n",
    "\n",
    "for col in range(0, 4):\n",
    "### Computing bins here\n",
    "    for c in numpy.unique(cl) :\n",
    "### Displaying each class at once, so they «overlap»\n",
    "\n",
    "### Displaying the legend and axis to «finish up» the figures.\n",
    "    axes[col].legend(loc='upper right')\n",
    "    axes[col].set_xlabel(iris.columns.values[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Computing the _centered_ scatter matrix :\n",
    "means = #### Computing the mean vector\n",
    "print(\"mean values : \", means)\n",
    "S = #### Computing the data scatter matrix\n",
    "print(\"Scatter matrix:\")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Performing the spectral decomposition\n",
    "import numpy.linalg\n",
    "pca_vals, pca_vects = ### Where is spectral decomposition in numpy ? (hint ^)\n",
    "print(\"PCA values :\", pca_vals)\n",
    "print(\"PCA directions :\\n\", pca_vects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scatter matrix _vs._ covariance matrix\n",
    "Indeed if we note $\\Sigma$ the coviarance matrix of the data set we then have\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{n-1}\\mathbf{S} $$\n",
    "\n",
    "Since the eigenvector decomposition of a matrix is scale invariant (only the eigenvalues are affected by the scale), one can perform this decomposition on the covariance matrix to get the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Sigma = numpy.cov(X.T) ## This is «also» performs the data centering\n",
    "pca_vals, pca_vects = ### Again …\n",
    "print(\"PCA values (cov):\", pca_vals)\n",
    "print(\"PCA values (scatter):\", pca_vals * (X.shape[0]-1)) ## barely rescaling the eigenvalues\n",
    "print(\"PCA directions :\\n\", pca_vects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Projection(s) on the PCA\n",
    "Performing the PCA of the dataset corresponds to finding an _adapted_ orthonormal basis for the dataset : _adapted_ here means that the basis is such that the scatter matrix of the dataset (in this basis) is diagonal.\n",
    "\n",
    "Once this (_partial_-)basis is found, the most natural thing is to apply this change of basis to the dataset. Since the (partial-)basis is orthonormal, this can simply be done by projecting the data on found vectors.\n",
    "\n",
    "**NB** : this indeed make sense on the _centered data_, not the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Computing the projection using a «simple» mamtrix product\n",
    "X_dec = (X - means) @ pca_vects\n",
    "print(X_dec[0:10, ::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Histograms on projections :\n",
    "fig, axes = pyplot.subplots(nrows=1, ncols=4, figsize=[15,4])\n",
    "\n",
    "for col in range(0, 4):\n",
    "### As above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### axis pairwise scatter plot\n",
    "\n",
    "numpy.array(pandas.factorize(iris['species']))\n",
    "\n",
    "pyplot.scatter(X_dec[::, 0], X_dec[::, 1], c=pandas.factorize(iris['species'])[0])\n",
    "pyplot.xlabel(\"PCA axis 1\")\n",
    "pyplot.ylabel(\"PCA axis 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## A bit better printout :\n",
    "pyplot.figure(figsize=[10,7])\n",
    "\n",
    "for c in numpy.unique(cl) :\n",
    "    pyplot.scatter(X_dec[cl==c, 0], X_dec[cl==c, 1], label=c, alpha=0.5)\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.xlabel(\"PCA axis 1\")\n",
    "pyplot.ylabel(\"PCA axis 2\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the anisotropic case\n",
    "When the native data, hence the _centered_ data, is not homogenuous a solution is to bring it to homogeneity in dimension by transforming each variable into a _dimension less_ value. This is typically done by dividing a variable by a value of the exact same _physical dimension_ (and unit). This produce _dimension less_ variables that express the characteristics of each datapoint relatively to a _scale_ that is _arbitrary_ for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Normalised/standardised data\n",
    "A standard way to find a _scale_ that can be applied to each variable is to take, for each variable, the _scale_ to be the **standard deviation** of this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Weighting\n",
    "Transforming each variable to a _dimension less_ variable also corresponds to setting a weigth on each variable such that they are all _in fine_ on a **common** _relative_ dimension. In this contexte, **normalisation/standardisatin** correspond to giving the same weight to every variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "USArrests = pandas.read_csv('USArrests.csv', header=0, sep=\";\", decimal=\",\")\n",
    "USArrests.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Isolating the variables (columns 0:4) and computing the scatter matrix\n",
    "Vars = USArrests.iloc[:, 1:5].values\n",
    "## From the classification of each sample \n",
    "states = USArrests.iloc[:, 0].values\n",
    "\n",
    "print(numpy.unique(states))\n",
    "print(USArrests.columns.values)\n",
    "print(Vars[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = pyplot.subplots(nrows=1, ncols=4, figsize=[15,4])\n",
    "\n",
    "for col in range(0, 4):\n",
    "### Display the 4 histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Computing the _centered_ scatter matrix :\n",
    "USA_means = ##### Complete here \n",
    "print(\"mean values : \", USA_means)\n",
    "USA_S = ##### Complete here\n",
    "print(\"Scatter matrix:\")\n",
    "print(USA_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### First not caring about _heterogeneity_ of dimensions\n",
    "No _standardisation_ (or _scaling_) of the data is performed before applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Performing the spectral decomposition\n",
    "import numpy.linalg\n",
    "pca_vals_no_scale, pca_vects_no_scale = numpy.linalg.eig(USA_S)\n",
    "print(\"PCA values :\", pca_vals_no_scale)\n",
    "print(\"PCA directions :\\n\", pca_vects_no_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Vars_dec_no_scale = #### Complete here\n",
    "print(Vars_dec_no_scale[0:10, ::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=[10, 7])\n",
    "pyplot.scatter(Vars_dec_no_scale[::, 0], Vars_dec_no_scale[::, 1])\n",
    "pyplot.xlabel(\"PCA axis 1\")\n",
    "pyplot.ylabel(\"PCA axis 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Performing some scalign\n",
    "The idea is to try to get all the initial variable on a joint _scale_, basically deviding each variable by its standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "USA_sds = numpy.std(Vars, axis=0)\n",
    "print(\"SD values : \", USA_sds)\n",
    "USA_S_scaled = ####\n",
    "print(\"Scatter matrix: (standardised)\")\n",
    "print(USA_S_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pca_vals_scaled, pca_vects_scaled = #### \n",
    "print(\"PCA values :\", pca_vals_scaled)\n",
    "print(\"PCA directions :\\n\", pca_vects_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Vars_dec_scaled = #### \n",
    "print(Vars_dec_scaled[0:10, ::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=[10, 7])\n",
    "pyplot.scatter(Vars_dec_scaled[::, 0], Vars_dec_scaled[::, 1])\n",
    "pyplot.xlabel(\"PCA axis 1\")\n",
    "pyplot.ylabel(\"PCA axis 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shortcut\n",
    "\n",
    "Indeed the ```scikit``` python module includes a direct implementation of the PCA (not in kit) : ```sklearn.decomposition.PCA```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-caad4caa7164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0miris_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0miris_proj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris_pca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "iris_pca = sklearn.decomposition.PCA()\n",
    "iris_proj = iris_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## A bit better printout :\n",
    "pyplot.figure(figsize=[10,7])\n",
    "\n",
    "for c in numpy.unique(cl) :\n",
    "    pyplot.scatter(iris_proj[cl==c, 0], iris_proj[cl==c, 1], label=c, alpha=0.5)\n",
    "for c in numpy.unique(cl) :\n",
    "    pyplot.scatter(X_dec[cl==c, 0], -X_dec[cl==c, 1], label=c, alpha=1, marker='.')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.xlabel(\"PCA axis 1\")\n",
    "pyplot.ylabel(\"PCA axis 2\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"PCA directions :\\n\", pca_vects)\n",
    "print(\"Using SK-learn version :\\n\", iris_pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
